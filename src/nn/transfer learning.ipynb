{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import os\n",
    "import os, sklearn, pandas, numpy as np, random\n",
    "from sklearn import svm\n",
    "import skimage, skimage.io, skimage.filters\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.utils import shuffle\n",
    "import imp\n",
    "\n",
    "# from pcanet import PCANet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tristan/Downloads/dog-breed-identification/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set cwd back to default\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset :: namedtuple(\n",
      "    ['train' = ['img_name']\n",
      "    , 'test' = ['img_name']\n",
      "    , 'validation' = ['img_name']\n",
      "    , 'labels' = pandas.df('img_name','classification')\n",
      "    , 'dict_index_to_label' = dict to convert label_index -> label_name\n",
      "    , 'dict_label_to_index'= dict to convert label_name -> label_index\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# custom scripts\n",
    "import config # params, constants\n",
    "import data, models # functions that mutate outr data\n",
    "# from utils import utils, plot # custom functions, in local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       cardboard\n",
      "1       cardboard\n",
      "2       cardboard\n",
      "3       cardboard\n",
      "4       cardboard\n",
      "5       cardboard\n",
      "6       cardboard\n",
      "7       cardboard\n",
      "8       cardboard\n",
      "9       cardboard\n",
      "10      cardboard\n",
      "11      cardboard\n",
      "12      cardboard\n",
      "13      cardboard\n",
      "14      cardboard\n",
      "15      cardboard\n",
      "16      cardboard\n",
      "17      cardboard\n",
      "18      cardboard\n",
      "19      cardboard\n",
      "20      cardboard\n",
      "21      cardboard\n",
      "22      cardboard\n",
      "23      cardboard\n",
      "24      cardboard\n",
      "25      cardboard\n",
      "26      cardboard\n",
      "27      cardboard\n",
      "28      cardboard\n",
      "29      cardboard\n",
      "          ...    \n",
      "2497        trash\n",
      "2498        trash\n",
      "2499        trash\n",
      "2500        trash\n",
      "2501        trash\n",
      "2502        trash\n",
      "2503        trash\n",
      "2504        trash\n",
      "2505        trash\n",
      "2506        trash\n",
      "2507        trash\n",
      "2508        trash\n",
      "2509        trash\n",
      "2510        trash\n",
      "2511        trash\n",
      "2512        trash\n",
      "2513        trash\n",
      "2514        trash\n",
      "2515        trash\n",
      "2516        trash\n",
      "2517        trash\n",
      "2518        trash\n",
      "2519        trash\n",
      "2520        trash\n",
      "2521        trash\n",
      "2522        trash\n",
      "2523        trash\n",
      "2524        trash\n",
      "2525        trash\n",
      "2526        trash\n",
      "Name: classification, Length: 2527, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import data # src/data.py\n",
    "dataset = data.init_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the amount of classes that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper', 'glass', 'plastic', 'metal', 'cardboard']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick the n classes with the most occuring instances\n",
    "amt = 5\n",
    "classes = data.top_classes(dataset.labels, amt)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "glass\n",
      "plastic\n",
      "metal\n",
      "cardboard\n",
      "paper\n",
      "glass\n",
      "plastic\n",
      "metal\n",
      "cardboard\n",
      "paper\n",
      "glass\n",
      "plastic\n",
      "metal\n",
      "cardboard\n"
     ]
    }
   ],
   "source": [
    "def extract_topx_classes(classes, train_or_test):\n",
    "    name_list = []\n",
    "    n_per_class = []\n",
    "    tail = '.jpg'\n",
    "    for cls in classes:\n",
    "        print(cls)\n",
    "        names = data.items_with_label(dataset.labels, cls)\n",
    "        if train_or_test == 'train':\n",
    "            train_names = [f for f in names if (f) in dataset.train]\n",
    "        if train_or_test == 'test':\n",
    "            train_names = [f for f in names if (f) in dataset.test]\n",
    "        if train_or_test == 'validation':\n",
    "            train_names = [f for f in names if (f) in dataset.validation]\n",
    "        name_list.append(train_names)\n",
    "        n_per_class.append(len(train_names))\n",
    "\n",
    "    n = min(n_per_class)\n",
    "    # (optional) reduce n to check whether the model can rember its input\n",
    "#     reduced_n = 50\n",
    "#     if n > reduced_n:    n = reduced_n\n",
    "    x = []\n",
    "    for ls in name_list:\n",
    "        for name in ls:\n",
    "            x.append(name)\n",
    "    random.shuffle(x)\n",
    "    return x\n",
    "\n",
    "x_train = extract_topx_classes(classes, 'train')\n",
    "x_test = extract_topx_classes(classes, 'test')\n",
    "x_validation = extract_topx_classes(classes, 'validation')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and convert images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extract all data:', 1940)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, n = data.extract_all(dataset, x_train)\n",
    "# y_test = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extract all data:', 300)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, n = data.extract_all_test(dataset, x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extract all data:', 150)\n"
     ]
    }
   ],
   "source": [
    "x_validation, y_validation, n  = data.extract_all_validation(dataset, x_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels and determine input and output shape for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset :: namedtuple(\n",
      "    ['train' = ['img_name']\n",
      "    , 'test' = ['img_name']\n",
      "    , 'validation' = ['img_name']\n",
      "    , 'labels' = pandas.df('img_name','classification')\n",
      "    , 'dict_index_to_label' = dict to convert label_index -> label_name\n",
      "    , 'dict_label_to_index'= dict to convert label_name -> label_index\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'data' from 'data.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test, y_validation = data.labels_to_vectors(dataset, y_train, y_test, y_validation)\n",
    "y_train = data.one_hot(y_train)\n",
    "y_test = data.one_hot(y_test)\n",
    "y_validation = data.one_hot(y_validation)\n",
    "input_shape = x_train.shape[1:] # = shape of an individual image (matrix)\n",
    "output_length = (y_train[0]).shape[0] # = length of an individual label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate VG16 network and add extra layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "\n",
    "def get_model(learn_rate=0.001, batches=10, dropout=0.10):\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(weights='imagenet', include_top=False, input_shape = input_shape )\n",
    "    print('Model loaded.')\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    # top_model = Sequential()\n",
    "    # top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    # top_model.add(Dense(256, activation='relu'))\n",
    "    # top_model.add(Dropout(0.5))\n",
    "    # top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    for layer in model.layers[0:13]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #Adding custom Layers \n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = (Dense(4096, activation='relu'))(x) #128\n",
    "    x = (Dropout(dropout))(x)\n",
    "    x = (Dense(4096, activation='relu'))(x) #128\n",
    "    x = (Dense(16, activation='relu'))(x)\n",
    "    # softmax output to get probability distribution\n",
    "    predictions = Dense(output_length, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "    # creating the final model \n",
    "    model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "    # Adam, SGD\n",
    "    # sgd = Keras.optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "    optimizer = optimizers.Adam(lr=learn_rate)\n",
    "\n",
    "    # compile the model \n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy',\n",
    "        'mean_squared_error','categorical_crossentropy','top_k_categorical_accuracy'])\n",
    "    \n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batches = [5, 10, 20]\n",
    "learn_rate = [0.0001, 0.001, 0.01]\n",
    "dropout = [0.1, 0.3, 0.5] # most used values\n",
    "param_grid = dict(learn_rate=learn_rate, batch_size=batches, dropout=dropout)\n",
    "\n",
    "# tune the hyperparameters via a randomized search\n",
    "model = KerasClassifier(build_fn=get_model, verbose=0)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid)\n",
    "start = time.time()\n",
    "grid.fit(x_validation, y_validation)\n",
    " \n",
    "# evaluate the best randomized searched model on the testing\n",
    "# data\n",
    "print(\"[INFO] randomized search took {:.2f} seconds\".format(\n",
    "    time.time() - start))\n",
    "acc = grid.score(x_validation, y_validation)\n",
    "print(\"[INFO] grid search accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(\n",
    "    grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n epochs = n iterations over all the training data\n",
    "batch_size = 5\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 622s 12s/step - loss: 1.9029 - acc: 0.2120 - mean_squared_error: 0.1692 - categorical_crossentropy: 1.9029 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "17/50 [=========>....................] - ETA: 6:40 - loss: 1.6094 - acc: 0.1529 - mean_squared_error: 0.1600 - categorical_crossentropy: 1.6094 - top_k_categorical_accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "model_final = get_model()\n",
    "\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "model_final.fit_generator(train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=50, epochs=epochs, callbacks=[TensorBoard(log_dir=config.tmp_model_dir)])\n",
    "        \n",
    "model.save(config.dataset_dir + 'models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "model_final.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          validation_split=1/6, callbacks=[TensorBoard(log_dir=config.tmp_model_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 384, 512, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 384, 512, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 384, 512, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 192, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 192, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 192, 256, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 96, 128, 128)      0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 96, 128, 256)      295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 96, 128, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 96, 128, 256)      590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 48, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 48, 64, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 48, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 48, 64, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 24, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 24, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 24, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 24, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 12, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 98304)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               12583040  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 27,299,877\n",
      "Trainable params: 22,024,421\n",
      "Non-trainable params: 5,275,456\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "model_final = get_model()\n",
    "model_final.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcanet import PCANet\n",
    "\n",
    "pcanet = PCANet(\n",
    "        image_shape= (512,348),\n",
    "        filter_shape_l1=2, step_shape_l1=1, n_l1_output=4,\n",
    "        filter_shape_l2=2, step_shape_l2=1, n_l2_output=4,\n",
    "        block_shape=2\n",
    "    )\n",
    "pcanet.validate_structure()\n",
    "\n",
    "pcanet.fit(x_train)\n",
    "X_train = pcanet.transform(x_train)\n",
    "X_test = pcanet.transform(x_test)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=1234, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy: \" + str(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
