{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import os\n",
    "import os, sklearn, pandas, numpy as np, random\n",
    "from sklearn import svm\n",
    "import skimage, skimage.io, skimage.filters\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.utils import shuffle\n",
    "import imp\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# from pcanet import PCANet\n",
    "from pcanet import PCANet\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tristan/Downloads/dog-breed-identification/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set cwd back to default\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset :: namedtuple(\n",
      "    ['train' = ['img_name']\n",
      "    , 'test' = ['img_name']\n",
      "    , 'validation' = ['img_name']\n",
      "    , 'labels' = pandas.df('img_name','label')\n",
      "    , 'dict_index_to_label' = dict to convert label_index -> label_name\n",
      "    , 'dict_label_to_index'= dict to convert label_name -> label_index\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# custom scripts\n",
    "import config # params, constants\n",
    "import data, models # functions that mutate outr data\n",
    "# from utils import utils, plot # custom functions, in local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data # src/data.py\n",
    "dataset = data.init_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the amount of classes that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper', 'glass', 'plastic', 'metal', 'cardboard']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick the n classes with the most occuring instances\n",
    "amt = 5\n",
    "classes = data.top_classes(dataset.labels, amt)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper\n",
      "glass\n",
      "plastic\n",
      "metal\n",
      "cardboard\n",
      "paper\n",
      "glass\n",
      "plastic\n",
      "metal\n",
      "cardboard\n",
      "paper\n",
      "glass\n",
      "plastic\n",
      "metal\n",
      "cardboard\n"
     ]
    }
   ],
   "source": [
    "def extract_topx_classes(classes, train_or_test):\n",
    "    name_list = []\n",
    "    n_per_class = []\n",
    "    tail = '.jpg'\n",
    "    for cls in classes:\n",
    "        print(cls)\n",
    "        names = data.items_with_label(dataset.labels, cls)\n",
    "        if train_or_test == 'train':\n",
    "            train_names = [f for f in names if (f) in dataset.train]\n",
    "        if train_or_test == 'test':\n",
    "            train_names = [f for f in names if (f) in dataset.test]\n",
    "        if train_or_test == 'validation':\n",
    "            train_names = [f for f in names if (f) in dataset.validation]\n",
    "        name_list.append(train_names)\n",
    "        n_per_class.append(len(train_names))\n",
    "\n",
    "    n = min(n_per_class)\n",
    "    # (optional) reduce n to check whether the model can rember its input\n",
    "#     reduced_n = 50\n",
    "#     if n > reduced_n:    n = reduced_n\n",
    "    x = []\n",
    "    for ls in name_list:\n",
    "        for name in ls:\n",
    "            x.append(name)\n",
    "    random.shuffle(x)\n",
    "    return x\n",
    "\n",
    "x_train = extract_topx_classes(classes, 'train')\n",
    "x_test = extract_topx_classes(classes, 'test')\n",
    "x_validation = extract_topx_classes(classes, 'validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and convert images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extract all data:', 1940)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, n = data.extract_all(dataset, x_train)\n",
    "# y_test = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extract all data:', 300)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, n = data.extract_all_test(dataset, x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extract all data:', 150)\n"
     ]
    }
   ],
   "source": [
    "x_validation, y_validation, n  = data.extract_all_validation(dataset, x_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels and determine input and output shape for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train, y_test, y_validation = data.labels_to_vectors(dataset, y_train, y_test, y_validation)\n",
    "y_train = data.one_hot(y_train)\n",
    "y_test = data.one_hot(y_test)\n",
    "y_validation = data.one_hot(y_validation)\n",
    "input_shape = x_train.shape[1:] # = shape of an individual image (matrix)\n",
    "output_length = (y_train[0]).shape[0] # = length of an individual label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1_train[0:10])\n",
    "print(y_train[0:10])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate VG16 network and add extra layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "\n",
    "def get_model(learn_rate=0.0001, batches=10, dropout=0.10):\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(weights='imagenet', include_top=False, input_shape = input_shape )\n",
    "    print('Model loaded.')\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    # top_model = Sequential()\n",
    "    # top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    # top_model.add(Dense(256, activation='relu'))\n",
    "    # top_model.add(Dropout(0.5))\n",
    "    # top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    for layer in model.layers[:13]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #Adding custom Layers \n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = (Dropout(dropout))(x)\n",
    "    x = (Dense(2048, activation='relu'))(x) #128\n",
    "    x = (Dense(2048, activation='relu'))(x) #128\n",
    "    x = (Dense(1024, activation='relu'))(x)\n",
    "    # softmax output to get probability distribution\n",
    "    predictions = Dense(output_length, activation=\"softmax\")(x) #activation=\"softmax\"\n",
    "\n",
    "\n",
    "    # creating the final model \n",
    "    model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "    # Adam, SGD\n",
    "    # sgd = Keras.optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "    optimizer = optimizers.Adam(lr=learn_rate)\n",
    "\n",
    "    # compile the model \n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy',\n",
    "        'mean_squared_error','categorical_crossentropy','top_k_categorical_accuracy'])\n",
    "    \n",
    "    return model_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batches = [5, 10, 20]\n",
    "learn_rate = [0.0001, 0.001, 0.01]\n",
    "dropout = [0.1, 0.3, 0.5] # most used values\n",
    "param_grid = dict(learn_rate=learn_rate, batch_size=batches, dropout=dropout)\n",
    "\n",
    "# tune the hyperparameters via a randomized search\n",
    "model = KerasClassifier(build_fn=get_model, verbose=0)\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid)\n",
    "start = time.time()\n",
    "grid.fit(x_validation, y_validation)\n",
    " \n",
    "# evaluate the best randomized searched model on the testing\n",
    "# data\n",
    "print(\"[INFO] randomized search took {:.2f} seconds\".format(\n",
    "    time.time() - start))\n",
    "acc = grid.score(x_validation, y_validation)\n",
    "print(\"[INFO] grid search accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(\n",
    "    grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    rotation_range = 30)\n",
    "\n",
    "validate_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    rotation_range = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n epochs = n iterations over all the training data\n",
    "batch_size = 10\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "194/194 [==============================] - 1931s 10s/step - loss: 1.2425 - acc: 0.4706 - mean_squared_error: 0.1290 - categorical_crossentropy: 1.2425 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "194/194 [==============================] - 1655s 9s/step - loss: 0.8740 - acc: 0.6433 - mean_squared_error: 0.0934 - categorical_crossentropy: 0.8740 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "194/194 [==============================] - 1675s 9s/step - loss: 0.7061 - acc: 0.7273 - mean_squared_error: 0.0750 - categorical_crossentropy: 0.7061 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "194/194 [==============================] - 1732s 9s/step - loss: 0.6044 - acc: 0.7675 - mean_squared_error: 0.0650 - categorical_crossentropy: 0.6044 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "194/194 [==============================] - 1645s 8s/step - loss: 0.5434 - acc: 0.8046 - mean_squared_error: 0.0562 - categorical_crossentropy: 0.5434 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "194/194 [==============================] - 1727s 9s/step - loss: 0.4939 - acc: 0.8237 - mean_squared_error: 0.0513 - categorical_crossentropy: 0.4939 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "194/194 [==============================] - 1662s 9s/step - loss: 0.4545 - acc: 0.8412 - mean_squared_error: 0.0468 - categorical_crossentropy: 0.4545 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "194/194 [==============================] - 1556s 8s/step - loss: 0.4264 - acc: 0.8582 - mean_squared_error: 0.0429 - categorical_crossentropy: 0.4264 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "194/194 [==============================] - 1490s 8s/step - loss: 0.3701 - acc: 0.8665 - mean_squared_error: 0.0388 - categorical_crossentropy: 0.3701 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "194/194 [==============================] - 1594s 8s/step - loss: 0.3810 - acc: 0.8758 - mean_squared_error: 0.0377 - categorical_crossentropy: 0.3810 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "194/194 [==============================] - 1619s 8s/step - loss: 0.3385 - acc: 0.8799 - mean_squared_error: 0.0356 - categorical_crossentropy: 0.3385 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "194/194 [==============================] - 1755s 9s/step - loss: 0.3144 - acc: 0.8887 - mean_squared_error: 0.0325 - categorical_crossentropy: 0.3144 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "194/194 [==============================] - 1540s 8s/step - loss: 0.2654 - acc: 0.9036 - mean_squared_error: 0.0272 - categorical_crossentropy: 0.2654 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "194/194 [==============================] - 1449s 7s/step - loss: 0.2626 - acc: 0.9119 - mean_squared_error: 0.0269 - categorical_crossentropy: 0.2626 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "194/194 [==============================] - 1434s 7s/step - loss: 0.2477 - acc: 0.9139 - mean_squared_error: 0.0258 - categorical_crossentropy: 0.2477 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da0b5e210>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final_augmentation = get_model()\n",
    "\n",
    "train_datagen.fit(x_train)\n",
    "val = validate_datagen.fit(x_validation)\n",
    "model_final_augmentation.fit_generator(train_datagen.flow(x_train, y_train, batch_size=batch_size), \n",
    "                                                           epochs=epochs, \n",
    "                                                           callbacks=[TensorBoard(log_dir=config.tmp_model_dir)])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1940/1940 [==============================] - 1173s 605ms/step - loss: 5.6430 - acc: 0.5840 - mean_squared_error: 0.1595 - categorical_crossentropy: 5.6430 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "1940/1940 [==============================] - 1246s 642ms/step - loss: 4.6797 - acc: 0.6840 - mean_squared_error: 0.1241 - categorical_crossentropy: 4.6797 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "1940/1940 [==============================] - 1241s 640ms/step - loss: 4.0543 - acc: 0.7232 - mean_squared_error: 0.1077 - categorical_crossentropy: 4.0543 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "1940/1940 [==============================] - 1288s 664ms/step - loss: 2.7610 - acc: 0.8015 - mean_squared_error: 0.0770 - categorical_crossentropy: 2.7610 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "1940/1940 [==============================] - 1221s 630ms/step - loss: 1.7671 - acc: 0.8562 - mean_squared_error: 0.0549 - categorical_crossentropy: 1.7671 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "1940/1940 [==============================] - 1265s 652ms/step - loss: 1.2024 - acc: 0.9005 - mean_squared_error: 0.0384 - categorical_crossentropy: 1.2024 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "1940/1940 [==============================] - 1136s 585ms/step - loss: 0.7777 - acc: 0.9361 - mean_squared_error: 0.0249 - categorical_crossentropy: 0.7777 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "1940/1940 [==============================] - 1063s 548ms/step - loss: 0.6186 - acc: 0.9448 - mean_squared_error: 0.0207 - categorical_crossentropy: 0.6186 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "1940/1940 [==============================] - 1070s 552ms/step - loss: 0.3795 - acc: 0.9619 - mean_squared_error: 0.0137 - categorical_crossentropy: 0.3795 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "1940/1940 [==============================] - 1075s 554ms/step - loss: 0.3026 - acc: 0.9716 - mean_squared_error: 0.0111 - categorical_crossentropy: 0.3026 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "1940/1940 [==============================] - 1055s 544ms/step - loss: 0.2431 - acc: 0.9778 - mean_squared_error: 0.0087 - categorical_crossentropy: 0.2431 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "1940/1940 [==============================] - 1090s 562ms/step - loss: 0.1933 - acc: 0.9809 - mean_squared_error: 0.0068 - categorical_crossentropy: 0.1933 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "1940/1940 [==============================] - 1072s 552ms/step - loss: 0.1519 - acc: 0.9830 - mean_squared_error: 0.0059 - categorical_crossentropy: 0.1519 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "1940/1940 [==============================] - 1086s 560ms/step - loss: 0.1482 - acc: 0.9866 - mean_squared_error: 0.0052 - categorical_crossentropy: 0.1482 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "1940/1940 [==============================] - 1073s 553ms/step - loss: 0.1786 - acc: 0.9820 - mean_squared_error: 0.0068 - categorical_crossentropy: 0.1786 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b97a752d4372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           validation_split=1/6, callbacks=[TensorBoard(log_dir=config.tmp_model_dir)])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'models/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_final = get_model()\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "model_final.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          validation_split=1/6, callbacks=[TensorBoard(log_dir=config.tmp_model_dir)])\n",
    "\n",
    "model.save(config.dataset_dir + 'models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model_final_augmentation.to_json()\n",
    "with open(config.dataset_dir + 'models/' + \"cnntransfer_augm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_final_augmentation.save_weights(config.dataset_dir + 'models/' + \"cnntransferweights_augmen.h5\", \"w\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
